{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982a9c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@d86d10\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@d86d10"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import java.time.Instant\n",
    "import java.time.ZoneId\n",
    "import java.time.format.DateTimeFormatter\n",
    "\n",
    "// Step 1: Initialize SparkSession\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"Time-Based Data Partitioning for Ratings\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67573942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratingsPath = gs://spark_learning_1/notebooks/ratings.csv\n",
       "ratingsDF = [userId: string, movieId: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userId: string, movieId: string ... 2 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Step 2: Load ratings.csv as a DataFrame from GCP\n",
    "val ratingsPath = \"gs://spark_learning_1/notebooks/ratings.csv\"\n",
    "val ratingsDF = spark.read.option(\"header\", \"true\").csv(ratingsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33fd56b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+----+\n",
      "|userId|movieId|rating|timestamp|year|\n",
      "+------+-------+------+---------+----+\n",
      "|     1|     17|   4.0|944249077|1999|\n",
      "|     1|     25|   1.0|944250228|1999|\n",
      "|     1|     29|   2.0|943230976|1999|\n",
      "|     1|     30|   5.0|944249077|1999|\n",
      "|     1|     32|   5.0|943228858|1999|\n",
      "|     1|     34|   2.0|943228491|1999|\n",
      "|     1|     36|   1.0|944249008|1999|\n",
      "|     1|     80|   5.0|944248943|1999|\n",
      "|     1|    110|   3.0|943231119|1999|\n",
      "|     1|    111|   5.0|944249008|1999|\n",
      "|     1|    161|   1.0|943231162|1999|\n",
      "|     1|    166|   5.0|943228442|1999|\n",
      "|     1|    176|   4.0|944079496|1999|\n",
      "|     1|    223|   3.0|944082810|1999|\n",
      "|     1|    232|   5.0|943228442|1999|\n",
      "|     1|    260|   5.0|943228696|1999|\n",
      "|     1|    302|   4.0|944253272|1999|\n",
      "|     1|    306|   5.0|944248888|1999|\n",
      "|     1|    307|   5.0|944253207|1999|\n",
      "|     1|    322|   4.0|944053801|1999|\n",
      "+------+-------+------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ratingsWithYearDF = [userId: string, movieId: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userId: string, movieId: string ... 3 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Step 3: Convert the timestamp field to a year column\n",
    "val ratingsWithYearDF = ratingsDF.withColumn(\n",
    "  \"year\",\n",
    "  year(from_unixtime(col(\"timestamp\").cast(\"long\")))\n",
    ")\n",
    "ratingsWithYearDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98ab3703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "validRatingsDF = [userId: string, movieId: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userId: string, movieId: string ... 3 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Step 4: Transformation - Filter out invalid or incomplete records\n",
    "val validRatingsDF = ratingsWithYearDF\n",
    "  .filter(col(\"userId\").isNotNull && col(\"movieId\").isNotNull && col(\"rating\").isNotNull && col(\"timestamp\").isNotNull)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc3c5a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "outputPath = hdfs:///user/casestudies/casestudy5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hdfs:///user/casestudies/casestudy5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val outputPath = \"hdfs:///user/casestudies/casestudy5\"\n",
    "validRatingsDF.limit(500000).write\n",
    "    .partitionBy(\"year\")\n",
    "    .format(\"parquet\").mode(\"overwrite\").save(outputPath)\n",
    "\n",
    "println(\"Execution finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f87fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
