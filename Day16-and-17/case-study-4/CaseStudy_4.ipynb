{"cells":[{"cell_type":"code","execution_count":1,"id":"56fc6ca8","metadata":{},"outputs":[{"data":{"text/plain":["spark = org.apache.spark.sql.SparkSession@76ddf55f\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["org.apache.spark.sql.SparkSession@76ddf55f"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.sql.SparkSession\n","\n","// Step 1: Initialize SparkSession\n","val spark = SparkSession.builder()\n","  .appName(\"Transfer File from GCS to HDFS\")\n","  .getOrCreate()"]},{"cell_type":"code","execution_count":2,"id":"b0cbfad1","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Transfer of file from GCS path: gs://spark_learning_1/notebooks/movies.csv to HDFS path: hdfs:///user/casestudies/casestudy4/movies.csv has been completed successfully!\n"]},{"data":{"text/plain":["gcsPath = gs://spark_learning_1/notebooks/movies.csv\n","hdfsPath = hdfs:///user/casestudies/casestudy4/movies.csv\n","data = [movieId: string, title: string ... 1 more field]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[movieId: string, title: string ... 1 more field]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["val gcsPath = \"gs://spark_learning_1/notebooks/movies.csv\"\n","val hdfsPath = \"hdfs:///user/casestudies/casestudy4/movies.csv\"\n","\n","val data = spark.read\n","  .option(\"header\", \"true\")  // Read the header from the CSV file\n","  .csv(gcsPath)\n","\n","// Step 4: Write the file to HDFS with headers\n","data.write\n","  .option(\"header\", \"true\")  // Include the header in the output\n","  .mode(\"overwrite\")         // Overwrite if the file already exists\n","  .csv(hdfsPath)\n","\n","println(s\"Transfer of file from GCS path: $gcsPath to HDFS path: $hdfsPath has been completed successfully!\")"]},{"cell_type":"code","execution_count":3,"id":"a3237c4e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1000 duplicates inserted and file updated successfully!\n"]},{"data":{"text/plain":["moviesPath = hdfs:///user/casestudies/casestudy4/movies.csv\n","moviesDF = [movieId: string, title: string ... 1 more field]\n","duplicateMoviesDF = [movieId: string, title: string ... 1 more field]\n","moviesWithDuplicatesDF = [movieId: string, title: string ... 1 more field]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[movieId: string, title: string ... 1 more field]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["//Adding duplicates\n","\n","// Step 1: Read the existing movies.csv from HDFS\n","val moviesPath = \"hdfs:///user/casestudies/casestudy4/movies.csv\"\n","val moviesDF = spark.read.option(\"header\", \"true\").csv(moviesPath)\n","\n","// Step 2: Add 1000 duplicates by appending the same DataFrame multiple times\n","val duplicateMoviesDF = moviesDF.limit(1000) // Take 1000 rows to duplicate\n","val moviesWithDuplicatesDF = moviesDF.union(duplicateMoviesDF) // Append duplicates\n","\n","// Step 3: Write the updated DataFrame with duplicates back to the same HDFS path\n","moviesWithDuplicatesDF.write\n","  .option(\"header\", \"true\")\n","  .mode(\"overwrite\") // Overwrite the existing file\n","  .csv(\"hdfs:///user/casestudies/casestudy4/duplicated_movies.csv\")\n","\n","println(\"1000 duplicates inserted and file updated successfully!\")"]},{"cell_type":"code","execution_count":4,"id":"d61eb711","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Marking org.apache.spark:spark-avro_2.12:3.3.2 for download\n","Obtained 12 files\n","Marking org.apache.spark:spark-avro_2.12:3.3.2 for download\n","Obtained 12 files\n"]}],"source":["// Add depedencies for avro format\n","%AddDeps org.apache.spark spark-avro_2.12 3.3.2 --transitive"]},{"cell_type":"code","execution_count":5,"id":"72b8fbcf","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading data from HDFS: hdfs:///user/casestudies/casestudy4/duplicated_movies.csv\n","+-------+--------------------+--------------------+\n","|movieId|               title|              genres|\n","+-------+--------------------+--------------------+\n","|      1|    Toy Story (1995)|Adventure|Animati...|\n","|      2|      Jumanji (1995)|Adventure|Childre...|\n","|      3|Grumpier Old Men ...|      Comedy|Romance|\n","|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n","|      5|Father of the Bri...|              Comedy|\n","+-------+--------------------+--------------------+\n","only showing top 5 rows\n","\n","Data loaded. Starting duplicate cleaning process...\n","Converting DataFrame to RDD and creating composite keys (movieId, title)...\n","Combining genres for duplicate movie entries...\n","Reformatting RDD back to DataFrame with unique movie entries...\n","Validation: Comparing original and deduplicated record counts...\n","Original record count: 88585\n","Deduplicated record count: 87585\n","Duplicates removed: 1000\n","Saving the cleaned data in Avro format to: hdfs:///user/casestudies/casestudy4/cleaned_movies.avro\n","Process complete. Cleaned data has been saved.\n"]},{"data":{"text/plain":["spark = org.apache.spark.sql.SparkSession@76ddf55f\n","moviesPath = hdfs:///user/casestudies/casestudy4/duplicated_movies.csv\n","moviesDF = [movieId: string, title: string ... 1 more field]\n","moviesRDD = MapPartitionsRDD[49] at map at <console>:64\n","uniqueMoviesRDD = ShuffledRDD[50] at reduceByKey at <console>:73\n","cleanedMoviesRDD = MapPartitionsRDD[51] at map at <console>:77\n","cleanedMoviesDF = [movieId: string, title: string ... 1 more field]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["originalCount:...\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[movieId: string, title: string ... 1 more field]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.sql.SparkSession\n","import org.apache.spark.sql.functions._\n","\n","// Step 1: Initialize SparkSession\n","val spark = SparkSession.builder()\n","  .appName(\"Movies Duplicate Cleaner\")\n","  .getOrCreate()\n","\n","// Step 2: Load movies.csv into a DataFrame from HDFS\n","val moviesPath = \"hdfs:///user/casestudies/casestudy4/duplicated_movies.csv\"\n","println(s\"Loading data from HDFS: $moviesPath\")\n","val moviesDF = spark.read.option(\"header\", \"true\").csv(moviesPath)\n","\n","// Displaying a preview of the dataset (optional, for debugging)\n","moviesDF.show(5)\n","\n","println(\"Data loaded. Starting duplicate cleaning process...\")\n","\n","// Step 3: Convert to RDD with composite key (movieId, title)\n","println(\"Converting DataFrame to RDD and creating composite keys (movieId, title)...\")\n","val moviesRDD = moviesDF.rdd.map(row => {\n","  val movieId = row.getString(row.fieldIndex(\"movieId\"))\n","  val title = row.getString(row.fieldIndex(\"title\"))\n","  val genres = row.getString(row.fieldIndex(\"genres\"))\n","  ((movieId, title), genres) // Key: (movieId, title), Value: genres\n","})\n","\n","// Combine genres for duplicate keys\n","println(\"Combining genres for duplicate movie entries...\")\n","val uniqueMoviesRDD = moviesRDD.reduceByKey((genres1, genres2) => s\"$genres1|$genres2\")\n","\n","// Transform back to (movieId, title, genres) format\n","println(\"Reformatting RDD back to DataFrame with unique movie entries...\")\n","val cleanedMoviesRDD = uniqueMoviesRDD.map {\n","  case ((movieId, title), combinedGenres) => (movieId, title, combinedGenres)\n","}\n","\n","val cleanedMoviesDF = cleanedMoviesRDD.toDF(\"movieId\", \"title\", \"genres\")\n","\n","// Step 4: Validation\n","println(\"Validation: Comparing original and deduplicated record counts...\")\n","val originalCount = moviesDF.count()\n","val deduplicatedCount = cleanedMoviesDF.count()\n","val duplicatesRemoved = originalCount - deduplicatedCount\n","println(s\"Original record count: $originalCount\")\n","println(s\"Deduplicated record count: $deduplicatedCount\")\n","println(s\"Duplicates removed: $duplicatesRemoved\")\n","\n","// Step 5: Store the final DataFrame in Avro format\n","val outputPath = \"hdfs:///user/casestudies/casestudy4/cleaned_movies.avro\"\n","println(s\"Saving the cleaned data in Avro format to: $outputPath\")\n","cleanedMoviesDF.write.format(\"avro\").save(outputPath)\n","\n","println(\"Process complete. Cleaned data has been saved.\")"]},{"cell_type":"code","execution_count":6,"id":"64bec44b","metadata":{},"outputs":[{"data":{"text/plain":["countDuplicates: (inputDF: org.apache.spark.sql.DataFrame)Long\n"]},"metadata":{},"output_type":"display_data"}],"source":["import org.apache.spark.sql.DataFrame\n","import org.apache.spark.sql.functions._\n","\n","/**\n"," * Method to count duplicate records in a DataFrame based on movieId and title.\n"," */\n","def countDuplicates(inputDF: DataFrame): Long = {\n","  // Group by 'movieId' and 'title' to count occurrences of each group\n","  val groupedDF = inputDF\n","    .groupBy(\"movieId\", \"title\")\n","    .count()\n","  \n","  // filter groups that have more than 1 record (duplicates)\n","  val duplicateGroupsDF = groupedDF.filter(col(\"count\") > 1)\n","  \n","  // Check if there are no duplicates\n","  if (duplicateGroupsDF.isEmpty) {\n","    println(\"No duplicate groups found.\")\n","    return 0L\n","  }\n","  \n","  \n","  val duplicateCount = duplicateGroupsDF\n","    .agg(sum(col(\"count\") - 1).alias(\"duplicateCount\")) // Subtract 1 for each group to exclude the first record\n","    .collect()\n","    .head\n","    .getLong(0)\n","  \n","  duplicateCount\n","}\n"]},{"cell_type":"code","execution_count":7,"id":"d4f9865d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["No duplicate groups found.\n"]},{"data":{"text/plain":["0"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["countDuplicates(cleanedMoviesDF)"]},{"cell_type":"code","execution_count":null,"id":"c77c3d88","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Apache Toree - Scala","language":"scala","name":"apache_toree_scala"},"language_info":{"codemirror_mode":"text/x-scala","file_extension":".scala","mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala","version":"2.12.15"}},"nbformat":4,"nbformat_minor":5}